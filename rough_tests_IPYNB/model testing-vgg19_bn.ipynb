{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_eval(model, test_path, test_class, transform):\n",
    "  ans = []\n",
    "  files = test_class.keys()\n",
    "  labels = test_class.values()\n",
    "  num_correct = 0\n",
    "  for file , idx in zip(files, labels):\n",
    "      path = os.path.join(test_path, file)\n",
    "      try:\n",
    "          img = Image.open(path)\n",
    "          tensor = transform(img)\n",
    "          temp_new_path = file.split('.')[0] + '.png'\n",
    "          new_path = os.path.join(test_path, temp_new_path)\n",
    "          img.save(new_path)\n",
    "          img_file = Image.open(new_path).convert('RGB')\n",
    "          tensor_img = transform(img_file)\n",
    "          tensor = tensor.unsqueeze(0).to(device)\n",
    "          output = model(tensor)\n",
    "          confidence, prediction = torch.max(output, 1)\n",
    "          ans.append(prediction.tolist()[0])\n",
    "          if prediction == idx:\n",
    "            num_correct += 1\n",
    "      except RuntimeError as e:\n",
    "          print('could not infer dtype of JPEG')\n",
    "  return ans, (num_correct/len(test_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_reults(model, transform, test_path, save_as):\n",
    "    preds = []\n",
    "    files = os.listdir(test_path)\n",
    "    for file in files:\n",
    "        path = os.path.join(test_path, file)\n",
    "        try:\n",
    "            img = Image.open(path)\n",
    "            temp_new_path = file.split('.')[0] + '.png'\n",
    "            new_path = os.path.join(test_path, temp_new_path)\n",
    "            img.save(new_path)\n",
    "            img_file = Image.open(new_path).convert('RGB')\n",
    "            tensor_image = transform(img_file)\n",
    "            tensor = tensor_image.unsqueeze(0).to(device)\n",
    "            output = model(tensor)\n",
    "            _, prediction = torch.max(output, 1)\n",
    "            preds.append(prediction.tolist()[0])\n",
    "        except RuntimeError as e:\n",
    "            print('could not infer dtype of JPEG, skipping ...')\n",
    "    \n",
    "    if save_as:\n",
    "        save_dict = {\n",
    "            'image_name: ': files,\n",
    "            'predicted_class': preds}\n",
    "        df = pd.DataFrame(save_dict)\n",
    "        df.to_csv(save_as)\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg19_path = '/home/anindya/Documents/kaggle/SAII/LAST TRIAL/models/VGG19.pth'\n",
    "vgg19_bn_path = '/home/anindya/Documents/kaggle/SAII/LAST TRIAL/models/VGG19_bn.pth'\n",
    "resnet_path = '/home/anindya/Documents/kaggle/SAII/LAST TRIAL/models/RESNET152.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torchvision.models import vgg19_bn, vgg19, resnet152"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "num_classes = 6\n",
    "model = vgg19_bn(pretrained=False)\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "model.classifier[6] = nn.Linear(model.classifier[6].in_features, num_classes)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the model\n",
    "\n",
    "pp = model.load_state_dict(torch.load(vgg19_bn_path, map_location=torch.device(device)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "test_path = '/home/anindya/Documents/kaggle/SAII/LAST TRIAL/test_folder'\n",
    "\n",
    "test_class = {'in1.jpg'    : 0,\n",
    "              'land3.jpg'  : 3,\n",
    "              'in3.jpeg'   : 0,\n",
    "              'earth1.jpeg': 0,\n",
    "              'd1.jpeg'    : 3,\n",
    "              'urb3.jpeg'  : 1,\n",
    "              'd2.jpeg'    : 3,\n",
    "              'urb2.jpeg'  : 1,\n",
    "              'earth3.jpeg': 0,\n",
    "              'land2.jpeg' : 3,\n",
    "              'sea1.jpg'   : 4,\n",
    "              'wild2.jpeg' : 1,\n",
    "              'sea3.jpeg'  : 4,\n",
    "              'earth4.jpeg': 0,\n",
    "              'in2.jpeg'   : 0,\n",
    "              'd4.jpg'     : 3,\n",
    "              'earth5.jpeg': 0,\n",
    "              'earth2.jpg' : 0,\n",
    "              'd3.jpg'     : 3,\n",
    "              'urb1.jpeg'  : 1,\n",
    "              'sea2.jpeg'  : 5,\n",
    "              'urb4.jpeg'  : 1,\n",
    "              'wild3.jpeg' : 1,\n",
    "              'wild1.jpg'  : 1,\n",
    "              'land1.jpg'  : 3}\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Resize((224,224)),\n",
    "                                transforms.Normalize(mean=[0.5,0.5,0.5], std=[0.5,0.5,0.5])\n",
    "                                ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "preds, acc = test_eval(model, test_path, test_class, transform)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "actual = list(test_class.values())\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "cm = confusion_matrix(actual, preds)\n",
    "sns.heatmap(cm, annot=True, cbar=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
